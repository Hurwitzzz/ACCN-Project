CPU/FPGA execution can be changed by commenting/uncommenting #define FPGA in boardnet.cpp
One might also need to comment out '-lpynq -lcma -lpthread' in the Makefile to build it on a machine where PYNQ_API isn't installed.
The Vitis include path needs to be put into the VitisPath file


Unfortunately an archive of the Vitis project was too big for moodle,
but the project setup is very barebones/standard anyways, as sources just conv.cpp and conv.h and for testbench just testbench.cpp (this one only tests
the implementation that runs on conv_test.dat in old/ , for testing the final implementation we used boardnet.cpp which has a corresponding task in the makefile

The expected folder structure of the folders not included here is
./weights
./data/conv_test.dat
./input_images

1. Implemented single Conv Layer working on FPGA. Our program can pass the lab3 test generated by gentest.py on FPGA.
	The implementation for this data (conv_test.dat) is in old/

2. Managed to do Tensor inference and Benchmark on Board.
	We can run mediumnet inference on the board and benchmark it
Run with CPU
    Running mediumnet on 1 images.
    input_images/n07753275_18098.bmp
    Image: input_images/n07753275_18098.bmp
    Predicted class with 0.630646 : pineapple, ananas 
    Actual Class: pineapple, ananas
    Total Time [ms]: 47079.164062
    Frames/s : 0.021241
    Execution Time [ms]:
    --------------------
    Linear Layer: 154.376002 
    Pool Layer: 0.000000 
    ReLU Layer: 0.058000 
    Conv Layer: 46924.441772 
    Softmax Layer: 0.262000 
    --------------------
    Accuracy: 1 of 1 images (1.000000 % )!
Run with FPGA:
    Running mediumnet on 1 images.
    input_images/n07753275_18098.bmp
    Image: input_images/n07753275_18098.bmp
    Predicted class with 0.630646 : pineapple, ananas 
    Actual Class: pineapple, ananas
    Total Time [ms]: 53147.753906
    Frames/s : 0.018815
    Execution Time [ms]:
    --------------------
    Linear Layer: 1211.516935 
    Pool Layer: 0.000000 
    ReLU Layer: 0.056000 
    Conv Layer: 51935.953613 
    Softmax Layer: 0.198000 
    --------------------
    Accuracy: 1 of 1 images (1.000000 % )!
Unfortunately we are still slower with the FPGA. This may be due to the 100Mhz clock count.
It could be potentially increased, but would need some refactoring to ease HLS into
splitting the loop over x, or perhaps splitting the loop manually, as currently the
negative slack would be too high.

3. Reuse data:
	a. For each kernel shift, we reuse the kernel_size * (kernel_size-1). 
	b. We didn't reuse data by shifting data within kernel, but replace them inplace, we using "modulo" to get the correct index for convolution.
	c. Calculating conv of different input channel leads to switch weight matrix between channels, reducing data reuse.
		To avoid this, we do iteration in this order: out_channel->out_height->in_channel->out_weight. This can reuse most of X and avoid switching W.

4. Create buffer like "acc_kernel", "acc_channel" to improve pipline. 

5. Compare to SW time of the same layer
	See the measurements above

6. Manages to inference classification on FPGA with MediumNet (without overlapping)

7. Full implementation with different datatype
	We tested synthesizing a design with a different datatype (see conv.h), but curiously
	found it to not affect the reported latency by a significant factor, so dropped it for now.

	The latency reported on Vitis HLS is a bit weird anyways. It doesn't really match up with
	the performance we eventually got when running the benchmark on the board.


-> How do we interface between CPU and FPGA:
0. We used PYNQ_API.
1. We allocated shared memory for W, B, Z of each conv layer with the size of th respective layer. For X we currently temporarily allocate shared memory during inference.
2. Pass the virtual pointers of shared memory to virtual pointers to copy data into shared memory. We only copy weights for one conv layer into shared_memory each time, in order to overlap CPU and FPGA (CPU load weight and FPGA do inference meanwhile). Another reason to do this is shared_memory will overflow if we want to load all weights one time.
3. Pass the physical_address of shared_memory to hls_buffer through axilite so that m_axi knows from where to get the data.
4. After inference of each layer, we transfer the result of ReLu back to CPU



-> Analyze difference in the two layers and how they differ from each other.
0. We inferenced with the complete mediumnet. There are layers with different number of input/output channels, different kernel_size, and different padding.
1. We used a template instantiation of the conv2d function for each layer, different layer requires different arguments for Conv2d().
	We split the network into blocks, each containing of Conv + Relu (+ optionally MaxPool)
2. Specializing statically over the different parameters leads to HLS being able to optimize more efficiently in our tests and generate
	smaller and more efficient code.
3. Since EntryConv() is our generic entry into the code running on the FPGA we branch off the block parameter and cast the arrays for each specific layer when we call Conv2D().
	HLS was able to generate better code when we turned all statically sized arrays into multi-dimensional statically sized ones.
4. Here are the raw results for the HLS synthesis of the various instantiations of Conv2D for every different convlayer/block of mediumnet
    +-------------------------------------------------------------------+--------+-------+-----------+-----------+----------+-----------+------+----------+----------+-----------+-------------+-------------+-----+
    |                              Modules                              |  Issue |       |  Latency  |  Latency  | Iteration|           | Trip |          |          |           |             |             |     |
    |                              & Loops                              |  Type  | Slack |  (cycles) |    (ns)   |  Latency |  Interval | Count| Pipelined|   BRAM   |    DSP    |      FF     |     LUT     | URAM|
    +-------------------------------------------------------------------+--------+-------+-----------+-----------+----------+-----------+------+----------+----------+-----------+-------------+-------------+-----+
    | + Conv2D_3_384_12_256_true_true_6_s                               |       -|   0.00|  140197907|  1.402e+09|         -|  140197907|     -|        no|    9 (3%)|    17 (7%)|    5969 (5%)|   6877 (12%)|    -|
    | + Conv2D_3_384_14_384_true_false_14_s                             |       -|   0.00|  257553427|  2.576e+09|         -|  257553427|     -|        no|    9 (3%)|    17 (7%)|    5589 (5%)|   6505 (12%)|    -|
    | + Conv2D_3_256_32_384_true_true_16_s                              |       -|   0.00|  561641491|  5.616e+09|         -|  561641491|     -|        no|   11 (3%)|    17 (7%)|    5830 (5%)|   6907 (12%)|    -|
    | + Conv2D_5_96_64_256_true_true_32_s                               |  Timing|  -1.12|  788918291|  7.889e+09|         -|  788918291|     -|        no|   11 (3%)|   27 (12%)|   10481 (9%)|  10863 (20%)|    -|
    | + Conv2D_7_3_128_96_true_true_64_s                                |  Timing|  -1.27|   47275315|  4.728e+08|         -|   47275315|     -|        no|    5 (1%)|   37 (16%)|  16668 (15%)|  15572 (29%)|    -|
Both input and output channels have a big impact on the latency,
Kernel size might have the biggest impact (see Conv2d_5 instantiation, kernel size 5) but luckily the block containing the biggest kernel has this impact offset
by only having 3 input channels (the pixels of the image)


-> Argue why you have moved certain layers to the FPGA
0. We moved the Conv and ReLU layer on FPGA, and partially the MaxPool layer too. We only maxpool "horizontally" on the FPGA as that allows us to still only
	keep one acc_row buffer while halving the DRAM writes to OUT. To finish the MaxPool layer we maxpool vertically on the CPU, as accessing memory in a non-contiguous pattern is faster there.
1. The implementation of Conv layer has a lot of for loop, which can be piplined on FPGA. Most importantly the looping over x can be pipelined with II=kernel_size.
2. To do Relu, we only need to say if the output of conv layer >0. It's convenient to do this on FPGA

