1. Managed to implement single Conv Layer working on FPGA. Our program can pass the lab3 test generated by gentest.py on FPGA.

2. Managed to do Tensor inference and Benchmark on Board.

3. Reuse data:
	a. For each kernel shift, we reuse the kerne_size * (kernel_size-1). 
	b. We didn't reuse data by shifting data within kernel, but replace them inplace, we using "modulo" to get the correct index 	for convolution.
	c. Calculating conv of different input channel leads to swith weight matrix between channels, reducing data reuse. To avoid 	this, we do iteration in this order: out_channel->out_height->in_channel->out_weight. This can reuse most of X and avoid 		switching W.

4. Create buffer like "acc_kernel", "acc_channel" to improve pipline. 

(not done yet) 5. Compare to SW time of the same layer

6. Manages to inference classification on FPGA with MediumNet (without overlapping)

(not done yet) 7. Full implementation with different datatype








-> How do we interface between CPU and FPGA:
0. We used PYNQ_API.
1. We allocated shared memory for X, W, B, Z of each conv layer. And each shared memory's size is equal to the max_size of layers in the MediumNet
2. Pass the virtual pointers of shared memorys to virtual pointers to copy data into shared memory. We only copy weights for one conv layer into shared_memory each time, in order to overlap CPU and FPGA (CPU load weight and FPGA do inference meanwhile). Another reason to do this is shared_memory will overflow if we want to load all weights one time.
3. Pass the physical_address of shared_memory to hls_buffer through axilite so that m_axi knows from where to get the data.
4. After inference of each layer, we transfer the result of ReLu back to CPU



-> Analyze difference in the two layers and how they differ from each other.
0. We inferenced with the complete mediumnet. There are layers with different number of input/output channels, different kernel_size, and different padding.
1. Because we hardcoded the conv2d function for each layer, different layer requires different arguments for Conv2d(). We use template to avoid rewriting code.
2. We pass MAX_INPUT_CHANNEL, MAX_IN_SIZE, etc. to EntryConv(), and cast arrays for each specific layer when we call Conv2D().




-> Argue why you have moved certain layers to the FPGA
0. We moved the Conv and ReLU layer on FPGA, and Pool on CPU
1. The implementation of Conv layer has a lot of for loop, which can be piplined on FPGA
2. To do Relu, we only need to say if the output of conv layer >0. It's convenient to do this on FPGA
3. Actually we planed to do Pool layer also on FPGA because it reduces half of data when transfering back to DRAM. But we couldn't finish it before ddl.

